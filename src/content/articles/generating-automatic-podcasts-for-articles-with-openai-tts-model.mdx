---
title: Generating artificial podcasts from articles with OpenAI's tts model
description: Exploring possible use of OpenAI's tts model for generating automatic "podcasts" for blog posts. I look into why would you want to do that and how did I make it.
slug: generating-artificial-podcasts-from-articles-with-openai-tts-model
pubDate: 2024-05-29
podcast: true
---

import CodeSnippet from "../../components/CodeSnippet.astro";
import Caption from "../../components/Caption.astro";

## Why am I doing this?

I love to listen to podcasts. Of course, making a quality podcast on your own is quiet a lot of work and I can't do that right now. As I need to rely on articles, and I still have some funds left on OpenAI's platform, I decided to try generating artificial "podcasts" from them.

### The plan

The idea is pretty simple! **We're going to use OpenAI's tts-1 model to generate speech files for my articles**. We'll try to make the process as automatic as possible by building a CLI tool for generating them. The tool needs to extract readable essence from our articles, so we need to process them in some way. Moreover, as calling OpenAI's API costs us money, we want to make sure it generates audio files only for new articles or the ones which've changed by implementing a simple cache.

## Building the CLI tool

Before we start, I just want to clarify that I'll focus on the "podcast" feature and caching in the following sections. I believe that the "glue code" itself is no that relevant here and everyone has different needs or expectations, so I won't spend time explaining how to read files or prompt user with `prompts` package.

### Extracting "readable" parts from MDX files

So, we need to process our MDX file in order to extract only the "readable" content. What does it mean actually? Well, we need to remove

- frontmatter,
- links or images,
- MDX features like JavaScript code and JSX components.

In fact, we can just get rid of everything by default and keep only what we want to keep. We're going to use `unified` and `remark` to parse our articles into ASTs. Let's install some packages!

<CodeSnippet>

```
npm i unified remarkFrontmatter remarkMDX remarkParse remarkStringify remarkUnlink unist-util-visit yaml
```

</CodeSnippet>

After installing the whole universe we can write a helper function that will return us tts input and slug for a given article, which is stored in frontmatter in my case. We'll use the slug for the name of generated `.mp3` file later. The following snippet shows how we can extract readable nodes having raw text content of our MDX file.

<CodeSnippet>

```js
import remarkFrontmatter from "remark-frontmatter";
import remarkMDX from "remark-mdx";
import remarkParse from "remark-parse";
import remarkUnlink from "remark-unlink";
import { unified } from "unified";
import { SKIP, visit } from "unist-util-visit";
import yaml from "yaml";

const READABLE_NODES = new Set(["paragraph", "list", "heading"]);

async function extractTTSInput(rawMDX) {
  const ast = await unified()
    // Generate AST
    .use(remarkUnlink)
    .run(
      unified()
        .use(remarkParse)
        .use(remarkMDX)
        .use(remarkFrontmatter)
        .parse(rawMDX),
    );

  // Extract frontmatter and "readable" nodes
  let frontmatter;
  const readableNodes = [];

  visit(ast, (node) => {
    if (node.type.startsWith("mdx")) {
      return SKIP;
    }

    if (READABLE_NODES.has(node.type)) {
      readableNodes.push(node);
      return SKIP;
    }

    if (node.type === "yaml") {
      frontmatter = yaml.parse(node.value);
    }
  });

  // Stringify AST back to text with injected title
  const ttsInput = unified().use(remarkStringify).stringify({
    type: "root",
    children: readableNodes,
  });

  return {
    ttsInput: `"${frontmatter.title}"\n${ttsInput}`,
    slug: frontmatter.slug,
  };
}
```

</CodeSnippet>

Essentially, we parse raw MDX article into AST, go node-by-node through it storing the "correct" ones in an array, and after that we stringify them to get our tts input. **It may turn out that there's more processing required in your case**. For example, if you have JSX nodes inside your paragraph nodes you might need to do some preprocessing to remove them from your AST as you can have problems with stringifying it later.

<CodeSnippet>

```json {4-13}
{
    "type": "paragraph",
    "children": [
      {
        "type": "mdxJsxTextElement",
        "name": "br",
        "attributes": [],
        "children": [],
        "position": {
          "start": { "line": 174, "column": 91, "offset": 6364 },
          "end": { "line": 174, "column": 96, "offset": 6369 }
        }
      },
    ],
},
```

<Caption slot="description">

In the previous snippet we traverse the AST recursively, and when we encounter `paragraph` node we just store it with all of its children. If you have `MDX` elements inside your paragraphs, you will keep them which will break `remarkStringify`. You might need additional pass in that case to remove all `mdx*` nodes first.

</Caption>

</CodeSnippet>

If you're a remark master my approach may be a little crude for you. I think it might be possible to write a custom plugin to do all of that. Personally, as a first-time user of unified and remark for such purpose, I think that the more primitive approach looks simpler. At the end of the day it's just a helper script.

### Generating audio files with OpenAI's tts-1 model

Now all we need to do is to call the OpenAI's audio API. As I am using Node.js I'll use the official npm package to do that.

<CodeSnippet>

```
npm i openai
```

</CodeSnippet>

Let's create an OpenAI client in our code and give it our precious API key. We can abstract the call itself into a function called `generatePodcast` which will return a Buffer with our `.mp3` file.

<CodeSnippet>

```js
import OpenAI from "openai";
import { Buffer } from "node:buffer";

if (!process.env.OPENAI_TTS_TOKEN) {
  throw new Error(
    "You need OPENAI_TTS_TOKEN env variable to generate audio files.",
  );
}

const openai = new OpenAI({
  apiKey: process.env.OPENAI_TTS_TOKEN,
});

async function generatePodcast(ttsInput) {
  const response = await openai.audio.speech.create({
    model: "tts-1",
    voice: "echo",
    input: ttsInput,
  });

  return Buffer.from(await response.arrayBuffer());
}
```

</CodeSnippet>

This is the secret sauce. Now we can compose `extractTTSInput` and `generatePodcast` functions together.

<CodeSnippet>

```js
const rawMDX = await fs.readFile("/teegardens-star-d", {
  encoding: "utf-8",
});

const { ttsInput, slug } = extractTTSInput(rawMDX);

await fs.writeFile(
  path.resolve(`public/assets/podcasts/${output.slug}.mp3`),
  await generatePodcast(output.ttsInput),
);
```

</CodeSnippet>

### Caching podcasts

Maybe the word _caching_ is a little exagerration here but still we want to do less work. I didn't want to come up with some groundbraking solutions so I just generate md5 hash for each tts input and link it with the article's filename in a json file named `podcasts.cache.json`. Having this I can check if an article has changed by comparing the current md5 hash with the cached one, and if so I can save it for further processing. Afterwards we update the cache with current md5 hashes of articles.

<CodeSnippet>

```js
for (const filename of articles) {
  const contents = await fs.readFile(filename, {
    encoding: "utf-8",
  });

  const { ttsInput } = await extractTTSInput(contents);

  const hash = createHash("md5").update(ttsInput).digest("hex");

  if (podcastsCache[filename] === hash) {
    // article didn't change, ignore it
    continue;
  }

  // save for further processing
}
```

</CodeSnippet>

Is it slow? Certainly not slower than waiting for OpenAI's API response, so we can neglect that.

To make sure I won't call the API for articles in which I've made an unsignificant change I prompt myself with the multiselect list of articles and generate "podcasts" only for the selected ones. The following snippet shows how it looks like.

<CodeSnippet>

```
✔ listing updated articles
? Pick articles for which you want to (re)generate podcast file. ›
move with arrows, toggle with space, submit with return
◯   generating-automatic-podcasts-for-articles-with-openai-tts-model.mdx
◯   some-other-article-which-doesn-t-exist-yet.mdx
```

</CodeSnippet>

## Final thoughts

I think that `tts-1` embodied in echo's voice does a really good job as an AI article reader. This feature would work especially well for less technical posts, where you can really just listen without looking at code snippets.

It's a bit unfortunate that the tts input for a single API call can have at most 4096 characters. Essentially it means that our script needs to handle cases in which our "readable" content exceeds this limit by breaking article into multiple audio files and joining them with `ffmpeg`. As I don't have to cope with this problem in this post I didn't solve it.

That's all for today! Thanks a ton for sticking around and reading through the entire article. If you would like to support me please feel free to drop a few bucks for a cup of coffe!
